傳統語法理論，現在已經被chatGPT完全取代掉了: https://zh.wikipedia.org/zh-hant/%E4%B9%94%E5%A7%86%E6%96%AF%E5%9F%BA%E8%B0%B1%E7%B3%BB，程式語言目前都是使用type2語法

a^n b^m，下面是type3語法描述方法

```
S -> a A    # 只能有一個非終端項目  # 所以不能 S -> A B
A -> bB | aA
B -> b | bB
```





分類問題

```
nn.CrossEntorpyLoss 做最後的損失函數
torch.optim.Adam 自動調整衝量的梯度下降法
```



回歸問題: 數值化問題



clustering: 讓電腦自己分群



perplexity 是衡量函數是否收斂的指標



DNN 分類範例

```python
# Some part of the code was referenced from below.
# https://github.com/pytorch/examples/tree/master/word_language_model 
import sys
import torch
import torch.nn as nn
import numpy as np
from torch.nn.utils import clip_grad_norm_
from data_utils import Dictionary, Corpus

# Device configuration
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

# Hyper-parameters
embed_size = 128
hidden_size = 1024
num_layers = 1
num_epochs = 3 # 原為 5
num_samples = 1000     # number of words to be sampled
batch_size = 20
seq_length = 30
learning_rate = 0.002

def load_data(train_file):
    global corpusObj, ids, vocab_size, num_batches
    corpusObj = Corpus()
    ids = corpusObj.get_data(train_file, batch_size)
    print('ids.shape=', ids.shape)
    vocab_size = len(corpusObj.dictionary)
    print('vocab_size=', vocab_size)
    num_batches = ids.size(1) // seq_length

# RNN based language model
class RNNLM(nn.Module):
    def __init__(self, method, vocab_size, embed_size, hidden_size, num_layers):
        super(RNNLM, self).__init__()
        method = method.upper()
        self.embed = nn.Embedding(vocab_size, embed_size)
        if method == "RNN":
            self.rnn = nn.RNN(embed_size, hidden_size, num_layers, batch_first=True)
        elif method == "GRU":
            self.rnn = nn.GRU(embed_size, hidden_size, num_layers, batch_first=True)
        else:
            raise Exception(f'RNNLM: method={method} not supported!')
        self.linear = nn.Linear(hidden_size, vocab_size)
        
    def forward(self, x, h):
        # Embed word ids to vectors
        x = self.embed(x)
        
        # Forward propagate 
        out, h = self.rnn(x, h)
        
        # Reshape output to (batch_size*seq_length, hidden_size)
        out = out.reshape(out.size(0)*out.size(1), out.size(2))
        
        # Decode hidden states of all time steps
        out = self.linear(out)
        return out, h

def train(corpus, method):
    global model
    model = RNNLM(method, vocab_size, embed_size, hidden_size, num_layers).to(device)

    # Loss and optimizer
    criterion = nn.CrossEntropyLoss()
    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)

    # Train the model
    for epoch in range(num_epochs):
        # Set initial hidden // and cell states (for LSTM)
        states = torch.zeros(num_layers, batch_size, hidden_size).to(device)
        
        for i in range(0, ids.size(1) - seq_length, seq_length):
            # Get mini-batch inputs and targets
            inputs = ids[:, i:i+seq_length].to(device) # 輸入為目前詞 (1-Batch)
            targets = ids[:, (i+1):(i+1)+seq_length].to(device) # 輸出為下個詞 (1-Batch)
            
            # Forward pass
            states = states.detach() # states 脫離 graph
            outputs, states = model(inputs, states) # 用 model 計算預測詞
            loss = criterion(outputs, targets.reshape(-1)) # loss(預測詞, 答案詞)
            
            # Backward and optimize
            optimizer.zero_grad() # 梯度歸零
            loss.backward() # 反向傳遞
            clip_grad_norm_(model.parameters(), 0.5) # 切斷，避免梯度爆炸
            optimizer.step() # 向逆梯度方向走一步

            step = (i+1) // seq_length
            if step % 100 == 0:
                print ('Epoch [{}/{}], Step[{}/{}], Loss: {:.4f}, Perplexity: {:5.2f}'
                    .format(epoch+1, num_epochs, step, num_batches, loss.item(), np.exp(loss.item())))

    # Save the model checkpoints
    # torch.save(model.state_dict(), 'model.ckpt')
    torch.save(model, f'{corpus}_{method}.pt')

def test(corpus, method):
    # Test the model
    model = torch.load(f'{corpus}_{method}.pt')
    with torch.no_grad():
        with open(f'{corpus}_{method}.txt', 'w', encoding='utf-8') as f:
            # Set intial hidden ane cell states
            state = torch.zeros(num_layers, 1, hidden_size).to(device)

            # Select one word id randomly # 這裡沒有用預熱
            prob = torch.ones(vocab_size)
            input = torch.multinomial(prob, num_samples=1).unsqueeze(1).to(device)

            for i in range(num_samples):
                # Forward propagate RNN 
                output, state = model(input, state)

                # Sample a word id
                prob = output.exp()
                word_id = torch.multinomial(prob, num_samples=1).item()

                # Fill input with sampled word id for the next time step
                input.fill_(word_id)

                # File write
                word = corpusObj.dictionary.idx2word[word_id]
                word = '\n' if word == '<eos>' else word + ' '
                f.write(word)

                if (i+1) % 100 == 0:
                    print('Sampled [{}/{}] words and save to {}'.format(i+1, num_samples, f'{corpus}_{method}.txt'))

if len(sys.argv) < 3:
    print('usage: python main.py <corpus> (train or test)')
    exit()

corpus = sys.argv[1]
method = sys.argv[2]
job = sys.argv[3]

load_data(f'{corpus}.txt')
if job == 'train':
    train(corpus, method)
elif job == 'test':
    test(corpus, method)
```

