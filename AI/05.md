03-人工智慧/04-神經網路/02-梯度



> vecGradient.py: 梯度(偏微分方向)下降

```sh
step = 0.01

# 我們想找函數 f 的最低點
def f(p):
    [x,y] = p
    return x * x + y * y

# df(f, p, k) 為函數 f 對變數 k 的偏微分: df / dp[k]
# 例如在上述 f 範例中 k=0, df/dx, k=1, df/dy
def df(f, p, k):
    p1 = p.copy()
    p1[k] += step
    return (f(p1) - f(p)) / step

# 函數 f 在點 p 上的梯度
def grad(f, p):
    gp = p.copy()
    for k in range(len(p)):
        gp[k] = df(f, p, k)
    return gp

[x,y] = [1,3]
print('x=', x, 'y=', y)
print('df(f(x,y), 0) = ', df(f, [x, y], 0))
print('df(f(x,y), 1) = ', df(f, [x, y], 1))
print('grad(f)=', grad(f, [x,y]))
```



梯度為0(梯度消失;遇到平地)就會停止，這是神經網路會遇到的問題，現在主要是使用加上動量處理，讓函數遇到平地還會向前走





反傳遞演算法會先正傳遞(前面的數推導到f)然後再返傳遞(由f推導到前面數字的梯度)

有了梯度之後我們會乘上一個很小的數，然後就沿著這個方向走下去

[ref](http://karpathy.github.io/neuralnets/?fbclid=IwAR229GfWD95boQ8LweXhC7KY4jcQiYLGJ_25qdeG0NT1UGBD2nWpl3bPwjg%EF%BC%8C%E7%B6%93%E9%81%8E%E4%BF%AE%E6%94%B9%E8%80%8C%E7%94%A2%E5%87%BA(%E8%A3%A1%E9%9D%A2%E5%AF%ABJavaScript%E6%88%91%E6%94%B9%E6%88%90python)%EF%BC%8C%E6%9C%AC%E6%96%87%E4%BD%BF%E7%94%A8%E7%9A%84%E5%9C%96%E7%89%87%E4%B9%9F%E6%98%AF%E5%BE%9E%E9%80%99%E8%A3%A1%E9%9D%A2%E7%8D%B2%E5%BE%97%E7%9A%84%EF%BC%8C%E8%A3%A1%E9%9D%A2%E6%9C%89%E6%B7%BB%E5%8A%A0%E4%B8%8A%E5%80%8B%E4%BA%BA%E4%B8%AD%E6%96%87%E5%BF%83%E5%BE%97%E8%A8%BB%E8%A7%A3%EF%BC%8C%E9%82%84%E6%9C%89%E8%AD%89%E6%98%8E%E9%83%A8%E5%88%86%E6%95%B8%E5%AD%B8%E5%81%8F%E5%BE%AE%E5%88%86(sigmoid))

使用python打造一個小的pytorch的範例: [Hacker's guide to Neural Networks (karpathy.github.io)](http://karpathy.github.io/neuralnets/?fbclid=IwAR229GfWD95boQ8LweXhC7KY4jcQiYLGJ_25qdeG0NT1UGBD2nWpl3bPwjg)

https://github.com/stereomp3/alg111a/tree/main/work/final

反傳遞的過程範例

> 乘法

$$
f(x,y) = x y
$$



就可以得出下方公式的結論: x 的偏為分為 y
$$
\frac{\partial f(x,y)}{\partial x} = \frac{f(x+h,y) - f(x,y)}{h} = \frac{(x+h)y - xy}{h} = \frac{xy + hy - xy}{h} = \frac{hy}{h} = y
$$


> 加法

$$
f(x,y) = x+y
$$

就可以得出下方公式的結論: x 和y 的偏為分為 1
$$
\frac{\partial f(x,y)}{\partial x} = \frac{f(x+h,y) - f(x,y)}{h} = \frac{(x+h+y) - (x+y)}{h} =  \frac{h}{h} = 1
$$


這種方法可以讓x和y計算偏微分時，減少很多運算，提升程式效能


$$
\frac{\partial f(u,z)}{\partial x} = \frac{\partial u(x,y)}{\partial x} \frac{\partial f(u,z)}{\partial u}
$$
因為x對u的偏微分為1，所以x和u的梯度比例是一樣的
$$
\frac{\partial u(x,y)}{\partial x} = 1、\frac{\partial u(x,y)}{\partial y} = 1
$$
所以u的梯度就等於x和y的梯度

```
set
f = u * z
y = x + y

x = 2
y = 3
z = 4

forward
u = 2+3 = 5
f = 4*5 = 20

backward
gu = z = 5
gz = u = 4
gx = gu*1 = 5
gy = gu*1 = 5
```



一開始正傳遞的時候，所有的梯度要設為0，然後再讓梯度進行累加，才可以讓神經網路運行

00000